

<!doctype html>
<html>

<head>


<title>Hongyuan Yu</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Hongyuan Yu, 俞宏远, CRIPAC, NLPR, CASIA, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, NKU"> 
<meta name="description" content="Hongyuan Yu's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
   function showPubs(id) {
  if (id == 0) {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
    document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select1').style = '';
  } else {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
    document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select0').style = '';
  }
}

</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Hongyuan Yu 俞宏远<h1>
				</div>

                <h3>Senior Algorithm Engineer </h3>

		<p>
                    Multimedia Department </br>
                    Xiaomi Inc. </br>
					</br>
                    Email: hongyuan_yu@yeah.net </br>
		</p>
		<p>
			<a href="https://github.com/hongyuanyu"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=yfnvzxYAAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://www.linkedin.com/in/hongyuan-yu-726126178/"><img src="assets/logos/linkedin_logo.png" height="30px"></a>&nbsp;&nbsp;
		</p>
			</td>

			</td>
			<td width="25%">
				<img src="assets/imgs/hongyuan.jpeg" width="100%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>News</h2>
<ul> 
<li> <p>2025.06.01, I was selected for the YOUNG ELITE SCIENTIST SPONSORSHIP PROGRAM BY CAST(科协青托). </a> </p></li> 
<li> <p>2025.05.01, One paper has been accepted bt IEEE Journal of Biomedical and Health Informatics. </a> </p></li>
<li> <p>2025.03.27, Our lighter version SPAN-F won the 2nd place in CVPR 2025 NTIRE's ESR Challenge, glad to see the top few teams were all inspired by SPAN.   </a> </p></li> 
<li> <p>2025.03.27, We won two third places in the CVPR2025 NTIRE competitions.</a> </p></li> 
<li> <p>2024.09.01, Because of my outstanding business output, I was promoted at Xiaomi. </a> </p></li> 
<li> <p>2024.06.20, I won the CVPR2024 NTIRE Challenge on Efficient Super-Resolution competition (2 champions) and the related paper has been accepted by the CVPR2024 NTIRE.</a> </p></li> 
<li> <p>2024.06.20, I won the CVPR2024 NTIRE Challenge on Image Super-Resolution (x4) competition.</a> </p></li> 
<li> <p>2023.08.01, I won the Quarterly Star of Xiaomi Multimedia Department.</a> </p></li> 
<li> <p>2023.06.20, I won the CVPR2023 MIPI RGBW Sensor Re-mosaic competition.</a> </p></li> 
<li> <p>2023.06.20, I won the CVPR2023 MIPI RGBW Sensor Fusion competition.</a> </p></li> 
<li> <p>2022.08.22, I have updated my personal website!</a> </p></li> 
</ul>

<h2>Biography</h2> 
<p>
Hongyuan Yu received the BSc degree from Nankai University (NKU) in 2017, and the PhD degree from University of Chinese Academy of Sciences (UCAS) in 2022, working with <a href="https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN">Liang Wang</a> and <a href="https://yanrockhuang.github.io/">Yan Huang</a>. Since July 2022, he has joined the <a href="https://www.mi.com/global/"></a>Multimedia Department, Xiaomi Inc as a senior algorithm engineer. He interned at Baidu Inc., Microsoft Research Asia, Microsoft Cloud AI and Ant Group.
</p>

<p>
His research interests include efficient deep learning, video object tracking, segmentation and detection, neural architecture search, model compression, etc. He was the valedictorian of Institute of Automation Chinese Academy of Sciences. He has obtained awards such as the Presidential Award of CAS, Excellent Graduate of Beijing, and ICDAR 2019 Best Paper Runner-up Award.
</p>

<h2> Selected Journal Papers</h2> 
<ul>
<li><p>Chao Fan <strong>Hongyuan Yu</strong>, Yan Huang, Liang Wang, Zhenghan Yang, and Xibin Jia, Slicemamba with neural architecture search for medical image segmentation, <i>IEEE Journal of Biomedical and Health Informatics (<strong>IEEE JBHI</strong>)</i>, accepted, 2025. <a href="https://arxiv.org/pdf/2407.08481" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu</strong>, Houwen Peng, Yan Huang, Hao Du, Jianlong Fu, Liang Wang, and Haibin Ling, Cyclic Differentiable Architecture Search, <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>)</i>, accepted, 2022. <a href="https://arxiv.org/pdf/2006.10724.pdf" target="_self">PDF</a> </p></li>

<li><p>Zerui Chen, Yan Huang, <strong>Hongyuan Yu</strong>, and Liang Wang, Learning a Robust Part-Aware Monocular 3D Human Pose Estimator via Neural Architecture Search, <i>International Journal of Computer Vision (<strong>IJCV</strong>)</i>, 130: 56–75, 2022. <a href="https://link.springer.com/article/10.1007/s11263-021-01525-0" target="_self">PDF</a> </p></li>  

<li><p>Chao Fan, <strong>Hongyuan Yu</strong>, Yan Huang, Caifeng Shan, Liang Wang, and Chenglong Li, SiamON: Siamese Occlusion-aware Network for Visual Tracking, <i>IEEE Transactions on Circuits and Systems for Video Technology (<strong>IEEE TCSVT</strong>)</i>, accepted, 2021. <a href="https://ieeexplore.ieee.org/document/9508452" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu</strong>, Yan Huang, Lihong Pi, Chengquan Zhang, Xuan Li, and Liang Wang, End-to-end Video Text Detection with Online Tracking, <i>Pattern Recognition (<strong>PR</strong>)</i>, accepted, 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032030594X" target="_self">PDF</a> </p></li>
</ul>
  
  
<h2>Selected Conference Papers</h2> 

<ul>
<li><p> Cheng Wan*, <strong>Hongyuan Yu*</strong>, Zhiqi Li*, Yihang Chen, Yajun Zou, Yuqing Liu, Xuanwu Yin and Kunlong Zuo, Swift Parameter-free Attention Network for Efficient Super-Resolution, <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (<strong>IEEE CVPRW</strong>)</i>, accepted, 2024. (<font color="#FF0000">Winer of NTIRE2024 ESR</font>) <a href="https://arxiv.org/pdf/2311.12770" target="_self">PDF</a> </p></li>

<li><p> Weichen Yu, <strong>Hongyuan Yu</strong>,  Yan Huang, and Liang Wang, Generalized Inter-class Loss for Gait Recognition, <i>ACM Conference on Multimedia (<strong>MM</strong>)</i>, accepted, 2022. <a href="" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu*</strong>, Tian Li*, Weichen Yu*, Jianguo Li, Yan Huang, Liang Wang, and Alex Liu, Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting, <i>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</i>, accepted, 2022. (<font color="#FF0000">Oral, 15% acceptance rate</font>)  <a href="" target="_self">PDF</a> </p></li>

<li><p>Wenmei Xu, <strong>Hongyuan Yu</strong>, Wei Wang and Liang Wang, Joint Learning Appearance and Motion Models for
  Visual Tracking, <i> Chinese Conference on Pattern Recognition and Computer Vision (<strong>PRCV</strong>)</i>, 2021.<a href="https://link.springer.com/chapter/10.1007/978-3-030-88004-0_34" target="_self">PDF</a> </p></li>

<li><p>Zerui Chen, Yan Huang, <strong>Hongyuan Yu</strong>, Bin Xue, Ke Han, Yiru Guo, and Liang Wang, Towards Part-aware Monocular 3D Human Pose Estimation: An Architecture Search Approach, <i>European Conference on Computer Vision (<strong>ECCV</strong>)</i>, accepted, 2020. (<font color="#FF0000">Spotlight, 5% acceptance rate</font>) <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480715.pdf" target="_self">PDF</a> </p></li>

<li><p>Houwen Peng*, Hao Du*, <strong>Hongyuan Yu*</strong>, Qi Li, Jing Liao, and Jianlong Fu, Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search, <i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, accepted, 2020. <a href="https://proceedings.neurips.cc//paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu*</strong>, Chengquan Zhang*, Xuan Li, Junyu Han, Errui Ding, and Liang Wang, An End-to-end Video
  Text Detector with Online Tracking, <i> International Conference on Document Analysis and Recognition (<strong>ICDAR</strong>)</i>, 2019. (<font color="#FF0000">Best Paper Runner-up Award, 0.5% acceptance rate</font>) <a href="https://ieeexplore.ieee.org/document/8978151" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu</strong>, Yan Huang, Lihong Pi and Liang Wang, Deconvolutional Generative Adversarial Networks
  with Application to Video Generation, <i> Chinese Conference on Pattern Recognition and Computer Vision (<strong>PRCV</strong>)</i>, 2019.<a href="https://link.springer.com/chapter/10.1007/978-3-030-31723-2_2" target="_self">PDF</a> </p></li>

</ul>


<h2>Competitions</h2> 
<ul>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ntire.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> CVPR2024 New Trends in Image Restoration and Enhancement workshop and associated challenges (NTIRE). 
      Our team (<b>Hongyuan Yu</b>, Wan Cheng, Yuxin Hong, Binnan Han, Zhuoyuan Wu, Yajun Zou, Yuqing Liu, Jizhe Li, Keji He, Chao Fan, Heng Zhang, Xiaolin Zhang, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">champion</font> of Efficient Super-Resolution competition (Main-Track and Runtime). 
      Our team (<b>Hongyuan Yu</b>, Wan Cheng, Yuxin Hong, Zhijuan Huang, Yajun Zou, Yuan Huang, Jiamin Lin, Xianyu Guan, Yongsheng Yu, Doan Zhang, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">champion</font> of Image Super-Resolution (x4) competition. 
      Our team Zhijuan Huang, (<b>Hongyuan Yu</b>, Cheng Wan, Wending Xiang, Jiamin Lin, Hang Zhong, Qiangsong Zhang, Yue Sun, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">runner-up</font> of RAW Image Super-Resolution. 
      Our team Binnan Han, (<b>Hongyuan Yu</b>, Wuzhuo Yuan, Wan Cheng, Yuqing Liu, Haodong Yu, Jizhe Li, Zhijuan Huang, Yajun Zou, Yuan Huang, Jiamin Lin, Xianyu Guan, Qi Jia, Heng Zhang, Xuanwu Yin, Kunlong Zuo) is the <font color="#FF0000">runner-up</font> of CVPR2024 AIS on Real-time Compressed Image Super-Resolution competition.
       See details here: <a href="https://cvlai.net/ntire/2024/">Results</a>.
    </p>
    </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/mipi.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> CVPR2023 Mobile intelligent photography and imaging (MIPI) Challenge. Our team (<b>Hongyuan Yu</b>, Yuqing Liu, Weichen Yu, Lin Ge, Binnan Han, Xiaolin Zhang, Zhen Dong, Xuanwu Yin, Kunlong Zuo)
       is the <font color="#FF0000">champion</font> of RGBW Sensor Fusion competition and <font color="#FF0000">champion</font> of RGBW Sensor Re-mosaic competition. See details here: <a href="https://mipi-challenge.org/MIPI2023/">Results</a>.
    </p>
    </td></tr></table>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ijcai_challenge.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 1st Learning and Mining with Noisy Labels Challenge. Our team (Weichen Yu, <b>Hongyuan Yu</b>, Yan Huang, Dong An,
      Keji He, Zhipeng Zhang, Xiuchuan Li, Liang Wang) is the <font color="#FF0000">runner-up</font> of task 1-1 and <font color="#FF0000">2nd runner-up</font> of task 1-2. See details here: <a href="https://yuankaiqi.github.io/REVERIE_Chalhttp://ucsc-real.soe.ucsc.edu:1995/Competition.html">Results</a>.
    </p>
    </td></tr></table>

<table class="imgtable"><tr><td>
  <img src="assets/logos/csig.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
  <td align="left">
  <p> REVERIE Challenge 2022. Our team TouchFish (Dong An, Yifeng Su, Shuanglin Sima, <b>Hongyuan Yu</b>, Weichen Yu, Yan Huang) is the <font color="#FF0000">runner-up</font> of both channels. See details here: <a href="https://yuankaiqi.github.io/REVERIE_Challenge/challenge_2022.html">Results of REVERIE Challenge 2022</a>.
  </p>
  </td></tr></table>

<table class="imgtable"><tr><td>
<img src="assets/logos/vot.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
<td align="left">
<p> VOT 2019: Visual Object Tracking Challenge. Our team (<b>Hongyuan Yu</b>, Houwen Peng, Zhirong Wu, Yan Huang,
Jianlong Fu, Liang Wang) is the <font color="#FF0000">champion</font> of the task: RGB-D. See details here: <a href="https://data.votchallenge.net/vot2019/presentations/vot2019_rgbd.pdf">Results of VOT 2019</a>.
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
  <img src="assets/logos/robocup.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
  <td align="left">
  <p> ROBOCUP JAPAN OPEN 2016. Our team ( Zhengdong Luo, Zihao An, Chengguang Xu, Fengting Li, Shuning Han, Yuanyuan Tong, Yanmei Jiao, <b>Hongyuan Yu</b>, Xiaotang Du) is the <font color="#FF0000">champion</font> of the task: Home and Simulation, the <font color="#FF0000">runner-up</font> of the task: Education. See details here: <a href="https://cc.nankai.edu.cn/2016/0405/c13291a147168/page.htm">Results of ROBOCUP JAPAN OPEN 2016</a>.
  </p>
  </td></tr></table>

</ul> 
  
  
<h2> Professional Activities</h2> 
<ul>
<li><p>Valedictorian, Institute of Automation Chinese Academy of Sciences (2022)</a></p></li>
<li><p>Chair, IEEE Student Branch, University of Chinese Academy of Sciences (2019-2020)</a></p></li>
<li><p>Reviewer, TPAMI, IJCV, CVPR, ECCV, ICML, ICLR, NeurIPS, AAAI, ACM MM, PR, ICIG, ICME, etc</p></li>
</ul>
  
  
<h2> Honors and Awards</h2> 
<ul> 
<li><p>2025, 入选2025中国科协<font color="#FF0000">青年人才托举工程</font></p> </li>
<li><p>2022, <font color="#FF0000">Valedictorian</font> of Institute of Automation Chinese Academy of Sciences (自动化所毕业生代表)</p> </li>
<li><p>2022, 中科院院长奖</p> </li>
<li><p>2022, 北京市优秀毕业生</p> </li>
<li><p>2021, 中国科学院自动化研究所攀登一等奖学金</p> </li>
<li><p>2020, 中国科学院大学IEEE学生分会主席贡献奖</p> </li>
<li><p>2019, <font color="#FF0000">Champion</font> of RGBD Object Tracking Challenge in the Workshop on VOT, ICCV 2019</p> </li>
<li><p>2019, 中国科学院大学三好学生</p> </li>
<li><p>2017, 中国科学院自动化研究所新生奖学金</p> </li>
<li><p>2016, Runner-up of ROBCUP JAPAN OPEN, 2016 </p> </li>
<li><p>2016, 南开大学公能奖学金 </p> </li>
<li><p>2016, 华为杯智能设计大赛三等奖 </p> </li>
<li><p>2015, Honorable Mention of MCM 2015 </p> </li>
<li><p>2014-2016, 国家励志奖学金</p> </li>
<li><p>2014-2016, 南开大学优秀学生干部</p> </li>

</ul>


<table width="100%"> 
	<tr> 
		<td align="center">&copy; Hongyuan Yu | Last update: Jun 2025</td>
	</tr> 
</table>

</div>


</body>

</html>

