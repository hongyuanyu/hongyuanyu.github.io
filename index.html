

<!doctype html>
<html>

<head>


<title>Hongyuan Yu</title>

<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="keywords" content="Hongyuan Yu, 俞宏远, CRIPAC, NLPR, CASIA, National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, NKU"> 
<meta name="description" content="Hongyuan Yu's home page">
<link rel="stylesheet" href="css/jemdoc.css" type="text/css" />

<script>
   function showPubs(id) {
  if (id == 0) {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_selected').innerHTML;
    document.getElementById('select0').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select1').style = '';
  } else {
    document.getElementById('pubs').innerHTML = document.getElementById('pubs_by_topic').innerHTML;
    document.getElementById('select1').style = 'text-decoration:underline;color:#000000';
    document.getElementById('select0').style = '';
  }
}

</script>

</head>


<body>

<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="75%">
				<div id="toptitle">
					<h1>Hongyuan Yu 俞宏远<h1>
				</div>

                <h3>Senior Algorithm Engineer</h3>

		<p>
                    Multimedia Department </br>
                    Xiaomi Inc. </br>
					</br>
                    Email: hongyuan_yu@yeah.net </br>
		</p>
		<p>
			<a href="https://github.com/hongyuanyu"><img src="assets/logos/github_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://scholar.google.com/citations?user=yfnvzxYAAAAJ&hl=zh-CN"><img src="assets/logos/google_logo.png" height="30px"></a>&nbsp;&nbsp;
			<a href="https://www.linkedin.com/in/hongyuan-yu-726126178/"><img src="assets/logos/linkedin_logo.png" height="30px"></a>&nbsp;&nbsp;
		</p>
			</td>

			</td>
			<td width="25%">
				<img src="assets/imgs/hongyuan.jpeg" width="100%"/>
			</td>
		<tr>
	</tbody>
</table>

<h2>News</h2>
<ul>
<li> <p>2022.08.22, I have updated my personal website!</a> </p></li> 
</ul>

<h2>Biography</h2> 
<p>
Hongyuan Yu received the BSc degree from Nankai University (NKU) in 2017, and the PhD degree from University of Chinese Academy of Sciences (UCAS) in 2022, working with <a href="https://scholar.google.com/citations?user=8kzzUboAAAAJ&hl=zh-CN">Liang Wang</a> and <a href="https://yanrockhuang.github.io/">Yan Huang</a>. Since July 2022, he has joined the <a href="https://www.mi.com/global/"></a>Multimedia Department, Xiaomi Inc as a senior algorithm engineer. He interned at Baidu Inc., Microsoft Research Asia, Microsoft Cloud AI and Ant Group.
</p>

<p>
His research interests include efficient deep learning, video object tracking, segmentation and detection, neural architecture search, model compression, etc. He was the valedictorian of Institute of Automation Chinese Academy of Sciences. He has obtained awards such as the Presidential Award of CAS, Excellent Graduate of Beijing, and ICDAR 2019 Best Paper Runner-up Award.
</p>

<h2> Selected Journal Papers</h2> 
<ul>
<li><p><strong>Hongyuan Yu</strong>, Houwen Peng, Yan Huang, Hao Du, Jianlong Fu, Liang Wang, and Haibin Ling, Cyclic Differentiable Architecture Search, <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE TPAMI</strong>)</i>, accepted, 2022. <a href="https://arxiv.org/pdf/2006.10724.pdf" target="_self">PDF</a> </p></li>

<li><p>Zerui Chen, Yan Huang, <strong>Hongyuan Yu</strong>, and Liang Wang, Learning a Robust Part-Aware Monocular 3D Human Pose Estimator via Neural Architecture Search, <i>International Journal of Computer Vision (<strong>IJCV</strong>)</i>, 130: 56–75, 2022. <a href="https://link.springer.com/article/10.1007/s11263-021-01525-0" target="_self">PDF</a> </p></li>  

<li><p>Chao Fan, <strong>Hongyuan Yu</strong>, Yan Huang, Caifeng Shan, Liang Wang, and Chenglong Li, SiamON: Siamese Occlusion-aware Network for Visual Tracking, <i>IEEE Transactions on Circuits and Systems for Video Technology (<strong>IEEE TCSVT</strong>)</i>, accepted, 2021. <a href="https://ieeexplore.ieee.org/document/9508452" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu</strong>, Yan Huang, Lihong Pi, Chengquan Zhang, Xuan Li, and Liang Wang, End-to-end Video Text Detection with Online Tracking, <i>Pattern Recognition (<strong>PR</strong>)</i>, accepted, 2021. <a href="https://www.sciencedirect.com/science/article/abs/pii/S003132032030594X" target="_self">PDF</a> </p></li>
</ul>
  
  
<h2>Selected Conference Papers</h2> 

<ul>
<li><p> Weichen Yu, <strong>Hongyuan Yu</strong>,  Yan Huang, and Liang Wang, Generalized Inter-class Loss for Gait Recognition, <i>ACM Conference on Multimedia (<strong>MM</strong>)</i>, accepted, 2022. <a href="" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu*</strong>, Tian Li*, Weichen Yu*, Jianguo Li, Yan Huang, Liang Wang, and Alex Liu, Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting, <i>International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</i>, accepted, 2022. (<font color="#FF0000">Oral, 15% acceptance rate</font>)  <a href="" target="_self">PDF</a> </p></li>

<li><p>Wenmei Xu, <strong>Hongyuan Yu</strong>, Wei Wang and Liang Wang, Joint Learning Appearance and Motion Models for
  Visual Tracking, <i> Chinese Conference on Pattern Recognition and Computer Vision (<strong>PRCV</strong>)</i>, 2021.<a href="https://link.springer.com/chapter/10.1007/978-3-030-88004-0_34" target="_self">PDF</a> </p></li>

<li><p>Zerui Chen, Yan Huang, <strong>Hongyuan Yu</strong>, Bin Xue, Ke Han, Yiru Guo, and Liang Wang, Towards Part-aware Monocular 3D Human Pose Estimation: An Architecture Search Approach, <i>European Conference on Computer Vision (<strong>ECCV</strong>)</i>, accepted, 2020. (<font color="#FF0000">Spotlight, 5% acceptance rate</font>) <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123480715.pdf" target="_self">PDF</a> </p></li>

<li><p>Houwen Peng*, Hao Du*, <strong>Hongyuan Yu*</strong>, Qi Li, Jing Liao, and Jianlong Fu, Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search, <i>Neural Information Processing Systems (<strong>NeurIPS</strong>)</i>, accepted, 2020. <a href="https://proceedings.neurips.cc//paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu*</strong>, Chengquan Zhang*, Xuan Li, Junyu Han, Errui Ding, and Liang Wang, An End-to-end Video
  Text Detector with Online Tracking, <i> International Conference on Document Analysis and Recognition (<strong>ICDAR</strong>)</i>, 2019. (<font color="#FF0000">Best Paper Runner-up Award, 0.5% acceptance rate</font>) <a href="https://ieeexplore.ieee.org/document/8978151" target="_self">PDF</a> </p></li>

<li><p><strong>Hongyuan Yu</strong>, Yan Huang, Lihong Pi and Liang Wang, Deconvolutional Generative Adversarial Networks
  with Application to Video Generation, <i> Chinese Conference on Pattern Recognition and Computer Vision (<strong>PRCV</strong>)</i>, 2019.<a href="https://link.springer.com/chapter/10.1007/978-3-030-31723-2_2" target="_self">PDF</a> </p></li>

</ul>


<h2>Competitions</h2> 
<ul>

  <table class="imgtable"><tr><td>
    <img src="assets/logos/ijcai_challenge.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
    <td align="left">
    <p> 1st Learning and Mining with Noisy Labels Challenge. Our team (Weichen Yu, <b>Hongyuan Yu</b>, Yan Huang, Dong An,
      Keji He, Zhipeng Zhang, Xiuchuan Li, Liang Wang) is the <font color="#FF0000">runner-up</font> of task 1-1 and <font color="#FF0000">2nd runner-up</font> of task 1-2. See details here: <a href="https://yuankaiqi.github.io/REVERIE_Chalhttp://ucsc-real.soe.ucsc.edu:1995/Competition.html">Results</a>.
    </p>
    </td></tr></table>

<table class="imgtable"><tr><td>
  <img src="assets/logos/csig.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
  <td align="left">
  <p> REVERIE Challenge 2022. Our team TouchFish (Dong An, Yifeng Su, Shuanglin Sima, <b>Hongyuan Yu</b>, Weichen Yu, Yan Huang) is the <font color="#FF0000">runner-up</font> of channel 2. See details here: <a href="https://yuankaiqi.github.io/REVERIE_Challenge/challenge_2022.html">Results of REVERIE Challenge 2022</a>.
  </p>
  </td></tr></table>

<table class="imgtable"><tr><td>
<img src="assets/logos/vot.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
<td align="left">
<p> VOT 2019: Visual Object Tracking Challenge. Our team (<b>Hongyuan Yu</b>, Houwen Peng, Zhirong Wu, Yan Huang,
Jianlong Fu, Liang Wang) is the <font color="#FF0000">champion</font> of the task: RGB-D. See details here: <a href="https://data.votchallenge.net/vot2019/presentations/vot2019_rgbd.pdf">Results of VOT 2019</a>.
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
  <img src="assets/logos/robocup.png" alt="alt text" width="80" height="80" /> &nbsp;</td>
  <td align="left">
  <p> ROBOCUP JAPAN OPEN 2016. Our team ( Zhengdong Luo, Zihao An, Chengguang Xu, Fengting Li, Shuning Han, Yuanyuan Tong, Yanmei Jiao, <b>Hongyuan Yu</b>, Xiaotang Du) is the <font color="#FF0000">champion</font> of the task: Home and Simulation, the <font color="#FF0000">runner-up</font> of the task: Education. See details here: <a href="https://cc.nankai.edu.cn/2016/0405/c13291a147168/page.htm">Results of ROBOCUP JAPAN OPEN 2016</a>.
  </p>
  </td></tr></table>

</ul> 
  
  
<h2> Professional Activities</h2> 
<ul>
<li><p>Valedictorian, Institute of Automation Chinese Academy of Sciences (2022)</a></p></li>
<li><p>Chair, IEEE Student Branch, University of Chinese Academy of Sciences (2019-2020)</a></p></li>
<li><p>Reviewer, ICLR, NeurIPS, ECCV, AAAI, ACM MM, PR, ICIG, ICME, etc</p></li>
</ul>
  
  
<h2> Honors and Awards</h2> 
<ul> 
<li><p>2022, Runner-up of Learning and Mining with Noisy Labels Challenge (IJCAI-ECAI)</p> </li>
<li><p>2022, Runner-up of REVERIE Challenge (CSIG)</p> </li>
<li><p>2022, <font color="#FF0000">Valedictorian</font> of Institute of Automation Chinese Academy of Sciences</p> </li>
<li><p>2022, 中科院院长奖</p> </li>
<li><p>2022, 北京市优秀毕业生</p> </li>
<li><p>2021, 中国科学院自动化研究所攀登一等奖学金</p> </li>
<li><p>2020, 中国科学院大学IEEE学生分会主席贡献奖</p> </li>
<li><p>2019, <font color="#FF0000">Champion</font> of RGBD Object Tracking Challenge in the Workshop on VOT, ICCV 2019</p> </li>
<li><p>2019, 中国科学院大学三好学生</p> </li>
<li><p>2017, 中国科学院自动化研究所新生奖学金</p> </li>
<li><p>2016, Runner-up of ROBCUP JAPAN OPEN, 2016 </p> </li>
<li><p>2016, 南开大学公能奖学金 </p> </li>
<li><p>2016, 华为杯智能设计大赛三等奖 </p> </li>
<li><p>2015, Honorable Mention of MCM 2015 </p> </li>
<li><p>2014-2016, 国家励志奖学金</p> </li>
<li><p>2014-2016, 南开大学优秀学生干部</p> </li>

</ul>


<table width="100%"> 
	<tr> 
		<td align="center">&copy; Hongyuan Yu | Last update: Aug 2022</td>
	</tr> 
</table>

</div>


</body>

</html>

